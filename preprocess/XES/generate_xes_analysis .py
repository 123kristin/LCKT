import os
import json
from tqdm import tqdm
from openai import OpenAI
import tiktoken

# ========== é…ç½® OpenAI ==========
client = OpenAI(
    api_key="YOUR API KEY",
    base_url="xxx"
)

# ========== TOKENIZER ==========
enc = tiktoken.get_encoding("cl100k_base")
MAX_TOKEN = 8192
SAFE_LIMIT = 8000  # ä¿é™©èµ·è§ï¼Œç”Ÿæˆæ—¶è¦æ±‚æ›´çŸ­

# ========== ç›®æ ‡ QID ==========
target_qids = {1386, 1593, 5669, 5862, 6504, 7121, 7134}

# ========== è·¯å¾„ ==========
json_path = "../data/XES3G5M/generate_analysis/Generated_XES_Analysis_temp.json"
save_path = "../data/XES3G5M/generate_analysis/Generated_XES_Analysis.json"

# ========== å·¥å…·å‡½æ•° ==========
def build_prompt(item):
    # é™åˆ¶ç”Ÿæˆå†…å®¹é•¿åº¦
    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“é€‰æ‹©é¢˜è§£æåŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä»¥ä¸‹é¢˜ç›®ç”Ÿæˆé«˜è´¨é‡çš„è§£æã€‚

è¦æ±‚ï¼š
- æ˜ç¡®æŒ‡å‡ºæ­£ç¡®ç­”æ¡ˆ
- è§£é‡Šä¸ºä»€ä¹ˆè¯¥ç­”æ¡ˆæ˜¯æ­£ç¡®çš„
- è§£é‡Šä¸ºä»€ä¹ˆå…¶ä»–é€‰é¡¹æ˜¯é”™è¯¯çš„
- ä½¿ç”¨æ¸…æ™°ç®€æ´çš„è¯­è¨€ï¼Œé€‚åˆå­¦ç”Ÿç†è§£
- å¦‚æœæœ‰æç¤ºä¿¡æ¯ï¼Œè¯·ç»“åˆæç¤ºè¿›è¡Œè§£é‡Š
- ç”¨è¦ç‚¹æˆ–ç®€çŸ­æ®µè½ç»„ç»‡ä½ çš„å›ç­”
- **é‡è¦**ï¼šä½ çš„è§£æå¿…é¡»ç®€æ´ï¼Œæ€»é•¿åº¦ä¸å¾—è¶…è¿‡{SAFE_LIMIT}ä¸ªtokens

é¢˜ç›®ï¼š
{item.get('content', '')}

é€‰é¡¹ï¼š
{item.get('choices', '')}

æç¤ºï¼š
{item.get('hint_text', '') if item.get('hint_text', '') else 'æ— '}
"""

def call_gpt(prompt, model="gpt-4o"):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful and knowledgeable educational assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[GPT Error] {e}")
        return None

def count_tokens(text):
    return len(enc.encode(text))

# ========== ä¸»æµç¨‹ ==========
if __name__ == "__main__":
    # 1. è¯»å–åŸå§‹æ•°æ®
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # 2. å¤„ç†ç›®æ ‡ QID
    qid2item = {int(item["questions"]): item for item in data}
    updated = 0

    for qid in tqdm(target_qids, desc="Regenerating explanations"):
        if qid not in qid2item:
            print(f"â— QID {qid} ä¸åœ¨æ•°æ®ä¸­ï¼Œè·³è¿‡ã€‚")
            continue
        item = qid2item[qid]
        prompt = build_prompt(item)
        explanation = call_gpt(prompt)
        if explanation is None:
            print(f"â— QID {qid} ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™åŸå†…å®¹ã€‚")
            continue
        # æ£€æŸ¥ç”Ÿæˆå†…å®¹tokenæ•°
        total_tokens = count_tokens(explanation)
        if total_tokens > MAX_TOKEN:
            print(f"âš ï¸ QID {qid} ç”Ÿæˆå†…å®¹ä»è¶…é•¿ï¼ˆ{total_tokens} tokensï¼‰ï¼Œå°†è‡ªåŠ¨æˆªæ–­ã€‚")
            tokens = enc.encode(explanation)
            explanation = enc.decode(tokens[:MAX_TOKEN])
        # æ›¿æ¢
        item["generated_explanation"] = explanation
        updated += 1
        print(f"âœ… QID {qid} è§£æå·²æ›´æ–°ï¼Œtokens: {min(total_tokens, MAX_TOKEN)}")
        print(f"QID: {qid}, keys: {list(item.keys())}")  # è°ƒè¯•ç”¨ï¼ŒæŸ¥çœ‹å®é™…å­—æ®µ

    # 3. ä¿å­˜åˆ°æ–°æ–‡ä»¶
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ å·²å®Œæˆ {updated} ä¸ª QID çš„è§£ææ›¿æ¢ï¼Œç»“æœå·²ä¿å­˜åˆ°ï¼š{save_path}")
